{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratoire 3 : Machines à vecteurs de support et réseaux neuronaux\n",
    "#### Département du génie logiciel et des technologies de l’information\n",
    "\n",
    "| Étudiants             |                                                         |\n",
    "|-----------------------|---------------------------------------------------------|\n",
    "| Jean-Philippe Decoste |  DECJ19059105                                           |\n",
    "| Ahmad Al-Taher        |   ALTA22109307                                          |\n",
    "| Stéphanie Lacerte     |   LACS06629109                                          |\n",
    "| Cours                 | GTI770 - Systèmes intelligents et apprentissage machine |\n",
    "| Session               | Automne 2018                                            |\n",
    "| Groupe                | 2                                                       |\n",
    "| Numéro du laboratoire | 02                                                      |\n",
    "| Professeur            | Hervé Lombaert                                          |\n",
    "| Chargé de laboratoire | Pierre-Luc Delisle                                      |\n",
    "| Date                  | 30 oct 2018                                             |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tabulate import tabulate\n",
    "\n",
    "from helpers import utilities as Utils\n",
    "from helpers import datasets as Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MERGED_GALAXY_PRIMITIVE = r\"data\\csv\\eq07_pMerged.csv\"\n",
    "ALL_GALAXY_PRIMITIVE = r\"data\\csv\\galaxy\\galaxy_feature_vectors.csv\"\n",
    "\n",
    "# Neural Network\n",
    "LAYERS_ACTIVATION = 'relu'\n",
    "LAST_LAYER_ACTIVATION = 'sigmoid'\n",
    "TENSORBOARD_SUMMARY = r\"tensorboard\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm():    \n",
    "    #linear    \n",
    "    print(\"SVM linear\")\n",
    "    c=[0.001,0.1,1.0,10.0]\n",
    "    params = dict(kernel=['linear'], C=c ,class_weight=['balanced'], cache_size=[2048])\n",
    "    grid = GridSearchCV(SVC(), param_grid=params, cv=dataset_splitter, n_jobs=-1, iid=True)\n",
    "    \n",
    "    #Fit the feature to svm algo\n",
    "    grid.fit(features_SVM, answers)\n",
    "    \n",
    "    #build table\n",
    "    outPut = []\n",
    "    for i in range(0, 4):\n",
    "        outPut.append([grid.cv_results_['params'][i]['C'],\n",
    "                          \"{0:.2f}%\".format(grid.cv_results_['mean_test_score'][i]*100)])\n",
    "    #print table\n",
    "    print(tabulate(outPut, headers=['Variable C','class_weight= {‘balanced’}']))\n",
    "    print(\"The best parameters are \", grid.best_params_,\" with a score of {0:.2f}%\".format(float(grid.best_score_)* 100))\n",
    "    \n",
    "    #rbf\n",
    "    print(\"\\nSVM rbf\")\n",
    "    params = dict(kernel=['rbf'], C=c, gamma=c ,class_weight=['balanced'], cache_size=[2048])\n",
    "    grid = GridSearchCV(SVC(), param_grid=params, cv=validation, n_jobs=-1, iid=True)\n",
    "    \n",
    "    #Fit the feature to svm algo\n",
    "    grid.fit(features_SVM, answers)\n",
    "    \n",
    "    #build table\n",
    "    outPut = []\n",
    "    outPut.append(['0.001',\n",
    "                   \"{0:.2f}%\".format(grid.cv_results_['mean_test_score'][0]*100),\n",
    "                   \"{0:.2f}%\".format(grid.cv_results_['mean_test_score'][1]*100),\n",
    "                   \"{0:.2f}%\".format(grid.cv_results_['mean_test_score'][2]*100),\n",
    "                   \"{0:.2f}%\".format(grid.cv_results_['mean_test_score'][3]*100)])\n",
    "    outPut.append(['0.1',\n",
    "                   \"{0:.2f}%\".format(grid.cv_results_['mean_test_score'][4]*100),\n",
    "                   \"{0:.2f}%\".format(grid.cv_results_['mean_test_score'][5]*100),\n",
    "                   \"{0:.2f}%\".format(grid.cv_results_['mean_test_score'][6]*100),\n",
    "                   \"{0:.2f}%\".format(grid.cv_results_['mean_test_score'][7]*100)])\n",
    "    outPut.append(['1.0',\n",
    "                   \"{0:.2f}%\".format(grid.cv_results_['mean_test_score'][8]*100),\n",
    "                   \"{0:.2f}%\".format(grid.cv_results_['mean_test_score'][9]*100),\n",
    "                   \"{0:.2f}%\".format(grid.cv_results_['mean_test_score'][10]*100),\n",
    "                   \"{0:.2f}%\".format(grid.cv_results_['mean_test_score'][11]*100)])\n",
    "    outPut.append(['10.0',\n",
    "                   \"{0:.2f}%\".format(grid.cv_results_['mean_test_score'][12]*100),\n",
    "                   \"{0:.2f}%\".format(grid.cv_results_['mean_test_score'][13]*100),\n",
    "                   \"{0:.2f}%\".format(grid.cv_results_['mean_test_score'][14]*100),\n",
    "                   \"{0:.2f}%\".format(grid.cv_results_['mean_test_score'][15]*100)])\n",
    "    \n",
    "    #print table\n",
    "    print(tabulate(outPut, headers=['Variable C','Ɣ=0.001','Ɣ=0.1','Ɣ=1.0','Ɣ=10.0']))\n",
    "    print(\"The best parameters are \", grid.best_params_,\" with a score of {0:.2f}%\".format(float(grid.best_score_)* 100))\n",
    "    \n",
    "    print(\"-> Done\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Réseaux neuronaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuralNetwork(runId, networkFrame, epoch, learning_rate):\n",
    "    # Format arrays to np arrays\n",
    "    features_train = []\n",
    "    answers_train = []\n",
    "    features_test = []\n",
    "    answers_test = []\n",
    "\n",
    "    for train_index, test_index in dataset_splitter.split(features, answers):\n",
    "        for elem in train_index:\n",
    "            features_train.append(features[elem])\n",
    "            answers_train.append(answers[elem])\n",
    "\n",
    "        for elem in test_index:\n",
    "            features_test.append(features[elem])\n",
    "            answers_test.append(answers[elem])\n",
    "\n",
    "    print(\"1.Initializing Neural Network for run #\" + str(runId))\n",
    "\n",
    "    # Create a default in-process session.\n",
    "    directory = TENSORBOARD_SUMMARY + \"/run\" + str(runId)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    print(\"TensorBoard summary writer at :\" + directory + \"\\n\")\n",
    "    tbCallBack = keras.callbacks.TensorBoard(log_dir=directory, histogram_freq=1, write_graph=False, write_images=False)\n",
    "    \n",
    "    # Parameters\n",
    "    dimension = len(features[0])\n",
    "    layers = networkFrame\n",
    "    epoch = epoch\n",
    "    batch_size = 200\n",
    "    learning_rate = learning_rate\n",
    "    \n",
    "    # The network type\n",
    "    neuralNetwork_model = keras.Sequential()\n",
    "    counter = 1\n",
    "\n",
    "    # Set layer in model\n",
    "    # First layer is set according to data dimension\n",
    "    neuralNetwork_model.add(keras.layers.Dense(dimension, input_dim=dimension, kernel_initializer='random_normal', bias_initializer='zeros', activation=LAYERS_ACTIVATION))\n",
    "    neuralNetwork_model.add(keras.layers.Dropout(0.5))\n",
    "    # Other layer set using layers array\n",
    "    for perceptron in layers:\n",
    "        if len(layers) == counter:\n",
    "            # Last layer (2 neurons for 2 possible class, SIGMOID ensure result between 1 and 0)\n",
    "            neuralNetwork_model.add(keras.layers.Dense(1, kernel_initializer='random_normal', bias_initializer='zeros', activation=LAST_LAYER_ACTIVATION))\n",
    "            #print(\"Layer #\" + str(counter) + \": dimension = \" + str(2) + \", activation = \" + LAST_LAYER_ACTIVATION)\n",
    "        else:\n",
    "            # Adds Layer\n",
    "            neuralNetwork_model.add(keras.layers.Dense(perceptron, kernel_initializer='random_normal', bias_initializer='zeros', activation=LAYERS_ACTIVATION))\n",
    "            neuralNetwork_model.add(keras.layers.Dropout(0.5))\n",
    "            #print(\"Layer #\" + str(counter) + \": dimension = \" + str(perceptron) + \", activation = \" + LAYERS_ACTIVATION)\n",
    "        counter = counter + 1\n",
    "\n",
    "    # Compile the network according to previous settings\n",
    "    neuralNetwork_model.compile(optimizer=tf.train.AdamOptimizer(learning_rate), \n",
    "                                loss='binary_crossentropy', \n",
    "                                metrics=['accuracy'])\n",
    "\n",
    "    # Print visualisation of network (layer and perceptron)\n",
    "    neuralNetwork_model.summary()\n",
    "\n",
    "    # Fit model to data\n",
    "    print(\"\\n2.Training\\n\")\n",
    "    neuralNetwork_model.fit(np.array(features_train), np.array(answers_train), \n",
    "                            epochs=epoch, \n",
    "                            batch_size=batch_size, \n",
    "                            validation_data=(np.array(features_test), np.array(answers_test)),\n",
    "                            callbacks=[tbCallBack], \n",
    "                            verbose=0)\n",
    "\n",
    "    # Evaluation\n",
    "    #scores = neuralNetwork_model.evaluate(np.array(features_train), np.array(answers_train), verbose=1)\n",
    "    #print(\"\\n%s: %.2f%%\" % (neuralNetwork_model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "    # Clear previous model\n",
    "    keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREPARING DATASETS\n",
      "Reading Galaxy features:\n",
      "Progress |**************************************************| 100.0% Complete\n",
      "-> Done\n",
      "\n",
      "Splitting Dataset according to these params:\n",
      "Property        Value\n",
      "------------  -------\n",
      "n_splits          5\n",
      "test_size         0.2\n",
      "random_state      0\n",
      "-> Done\n",
      "\n",
      "\n",
      "ALGORITHMS\n",
      "\n",
      "SVM:\n",
      "\n",
      "Neural Network:\n",
      "TensorFlow version:1.12.0, Keras version:2.1.6-tf\n",
      "\n",
      "1.Initializing Neural Network for run #1\n",
      "TensorBoard summary writer at :tensorboard/run1\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 78)                6162      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 78)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               7900      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 24,263\n",
      "Trainable params: 24,263\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "2.Training\n",
      "\n",
      "1.Initializing Neural Network for run #2\n",
      "TensorBoard summary writer at :tensorboard/run2\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 78)                6162      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 78)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               7900      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 150)               15150     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 80)                12080     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 30)                2430      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 31        \n",
      "=================================================================\n",
      "Total params: 43,753\n",
      "Trainable params: 43,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "2.Training\n",
      "\n"
     ]
    },
    {
     "ename": "SystemError",
     "evalue": "<built-in function EventsWriter_Flush> returned a result with an error set",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\gti770_env\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m    613\u001b[0m     \u001b[0m__swig_setmethods__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 614\u001b[1;33m     \u001b[0m__setattr__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m_swig_setattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mStatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    615\u001b[0m     \u001b[0m__swig_getmethods__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-5e67b0962385>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"TensorFlow version:\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVERSION\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\", Keras version:\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mneuralNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m60\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0005\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mneuralNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m150\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m80\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.005\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mneuralNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m120\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m80\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m80\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.05\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-0b7c0e2a4f1e>\u001b[0m in \u001b[0;36mneuralNetwork\u001b[1;34m(runId, networkFrame, epoch, learning_rate)\u001b[0m\n\u001b[0;32m     67\u001b[0m                             \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswers_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtbCallBack\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                             verbose=0)\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;31m# Evaluation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gti770_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1637\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1638\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1639\u001b[1;33m           validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1641\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gti770_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    219\u001b[0m           \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m           \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gti770_env\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[0mt_before_callbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m       \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m     \u001b[0mdelta_t_median\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gti770_env\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1076\u001b[0m                   \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1077\u001b[0m                   if k not in ['batch', 'size', 'num_steps']}\n\u001b[1;32m-> 1078\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_write_custom_summaries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_total_batches_seen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1079\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_total_batches_seen\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gti770_env\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_write_custom_summaries\u001b[1;34m(self, step, logs)\u001b[0m\n\u001b[0;32m   1051\u001b[0m         \u001b[0msummary_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1053\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1054\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1055\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gti770_env\\lib\\site-packages\\tensorflow\\python\\summary\\writer\\writer.py\u001b[0m in \u001b[0;36mflush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    393\u001b[0m     \u001b[0mdisk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m     \"\"\"\n\u001b[1;32m--> 395\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevent_writer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    396\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    397\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gti770_env\\lib\\site-packages\\tensorflow\\python\\summary\\writer\\event_file_writer.py\u001b[0m in \u001b[0;36mflush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    117\u001b[0m     \"\"\"\n\u001b[0;32m    118\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ev_writer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFlush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gti770_env\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mFlush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    697\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mFlush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_pywrap_tensorflow_internal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEventsWriter_Flush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    700\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mClose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSystemError\u001b[0m: <built-in function EventsWriter_Flush> returned a result with an error set"
     ]
    }
   ],
   "source": [
    "#1.A Read Galaxy features (name of file, path, n_split, test size, random state)\n",
    "if os.path.isfile(MERGED_GALAXY_PRIMITIVE):\n",
    "    features, features_SVM, answers, dataset_splitter = Data.prepareDataset(\"Galaxy\", MERGED_GALAXY_PRIMITIVE, 5, 0.2, 0)\n",
    "else:\n",
    "    features, features_SVM, answers, dataset_splitter = Data.prepareDataset(\"Galaxy\", ALL_GALAXY_PRIMITIVE, 5, 0.2, 0)\n",
    "\n",
    "#2. Algorithms\n",
    "print(\"ALGORITHMS\")\n",
    "print(\"\\nSVM:\")\n",
    "#svm()\n",
    "\n",
    "print(\"\\nNeural Network:\")\n",
    "print(\"TensorFlow version:\" + tf.VERSION + \", Keras version:\" + tf.keras.__version__ + \"\\n\")\n",
    "# Diff number of layer\n",
    "neuralNetwork(1, [100, 100, 2], 60, 0.0005)\n",
    "neuralNetwork(2, [100, 2], 60, 0.0005)\n",
    "neuralNetwork(3, [100, 100, 100, 100, 2], 60, 0.0005)\n",
    "# Diff perceptron\n",
    "neuralNetwork(4, [80, 50, 2], 60, 0.0005)\n",
    "neuralNetwork(5, [120, 2], 60, 0.0005)\n",
    "neuralNetwork(6, [100, 120, 100, 50, 2], 60, 0.0005)\n",
    "# Diff epoch\n",
    "neuralNetwork(7, [100, 100, 2], 60, 0.0005)\n",
    "neuralNetwork(8, [100, 2], 20, 0.0005)\n",
    "neuralNetwork(9, [100, 100, 100, 100, 2], 100, 0.0005)\n",
    "# Diff learning\n",
    "neuralNetwork(10, [100, 100, 2], 60, 0.0005)\n",
    "neuralNetwork(11, [100, 2], 60, 0.005)\n",
    "neuralNetwork(12, [100, 100, 100, 100, 2], 60, 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REMOVE NEXT CELL IN FINAL VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output \n",
    "SVM linear\n",
    "  Variable C  class_weight= {‘balanced’}\n",
    "------------  ----------------------------\n",
    "       0.001  78.84%\n",
    "       0.1    80.09%\n",
    "       1      80.00%\n",
    "      10      79.99%\n",
    "The best parameters are  {'C': 0.1, 'cache_size': 2048, 'class_weight': 'balanced', 'kernel': 'linear'}  with a score of 80.09%\n",
    "SVM rbf\n",
    "  Variable C  Ɣ=0.001    Ɣ=0.1    Ɣ=1.0    Ɣ=10.0\n",
    "------------  ---------  -------  -------  --------\n",
    "       0.001  51.89%     68.10%   51.89%   51.89%\n",
    "       0.1    73.39%     80.66%   80.40%   78.23%\n",
    "       1      79.90%     78.23%   82.70%   82.38%\n",
    "      10      79.35%     81.68%   83.66%   82.12%\n",
    "The best parameters are  {'C': 10.0, 'cache_size': 2048, 'class_weight': 'balanced', 'gamma': 0.1, 'kernel': 'rbf'}  with a score of 83.66%\n",
    "-> Done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 \n",
    "### Présentation de la méthode de validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "### Présentation de la méthode de normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "### Description du réseau de neuronnes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "### Analyse du réseau de neuronnes grace aux graphiques TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "### Score F1 et impact des hyperparamètres sur le réseau de neuronnes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "### Méthode d'optimisation du modèle SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "### Impact de la taille de l'ensemble d'apprentissage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "### Formulation des recommandations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9\n",
    "### Améliorations possibles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
