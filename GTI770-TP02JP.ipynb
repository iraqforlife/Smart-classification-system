{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratoire 2 : Arbre de désision, Bayes naïf et KNN\n",
    "#### Département du génie logiciel et des technologies de l’information\n",
    "\n",
    "| Étudiants             |                                                         |\n",
    "|-----------------------|---------------------------------------------------------|\n",
    "| Jean-Philippe Decoste |  DECJ19059105                                           |\n",
    "| Ahmad Al-Taher        |   ALTA22109307                                          |\n",
    "| Cours                 | GTI770 - Systèmes intelligents et apprentissage machine |\n",
    "| Session               | Automne 2018                                            |\n",
    "| Groupe                | 2                                                       |\n",
    "| Numéro du laboratoire | 02                                                      |\n",
    "| Professeur            | Hervé Lombaert                                          |\n",
    "| Chargé de laboratoire | Pierre-Luc Delisle                                      |\n",
    "| Date                  | 30 oct 2018                                             |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes et librairies utilisées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "import os\n",
    "\n",
    "import graphviz\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from image import Image as imageObj\n",
    "from imageV2 import Image as imageFeat\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV, ShuffleSplit, StratifiedShuffleSplit\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from spam import Spam\n",
    "import utilities\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paramètres de l'application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LAB01\n",
    "IMAGE_CSV_NAME = r\"data\\csv\\galaxy\\galaxy_label_data_set.csv\"\n",
    "IMAGETEST_CSV_NAME = r\"data\\csv\\galaxy\\galaxy_label_data_set_test.csv\"\n",
    "\n",
    "#Galaxy features\n",
    "EXTRATED_GALAXY_PRIMITIVE = r\"data\\csv\\eq07_pExtraction.csv\"\n",
    "MERGED_GALAXY_PRIMITIVE = r\"data\\csv\\eq07_pMerged.csv\"\n",
    "ALL_GALAXY_PRIMITIVE = r\"data\\csv\\galaxy\\galaxy_feature_vectors.csv\"\n",
    "\n",
    "#Spam features\n",
    "ALL_SPAM_PRIMITIVE = r\"data\\csv\\spam\\spam.csv\"\n",
    "\n",
    "#Algo params\n",
    "TREE_DEPTH = [None, 3, 5, 10]\n",
    "EXTRACT_TREE_PDF = False\n",
    "N_NEIGHBORS = [3, 5, 10]\n",
    "WEIGHTS = ['uniform', 'distance']\n",
    "VALIDATION_METHOD = ['Holdout', 'Stratified Shuffle Split']\n",
    "\n",
    "#General config\n",
    "PRIMITIVE_SCANNING = True\n",
    "DOMERGE = False\n",
    "PRINT_GRAPH = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arbre de décision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decisionTree():\n",
    "    \"\"\"\n",
    "    Algorithme 'Decision Tree' utilisé pour classer les données qui lui sont fourni\n",
    "    Args:\n",
    "        --\n",
    "    \"\"\"\n",
    "    validationCounter = 1\n",
    "    for method in VALIDATION_METHOD:\n",
    "        validation =  None\n",
    "        if method == 'Holdout':\n",
    "            validation = ShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
    "        elif method == 'Stratified Shuffle Split':\n",
    "            validation = StratifiedShuffleSplit()\n",
    "        \n",
    "        print(str(validationCounter)+\".Training with \"+method+\"\\n\")\n",
    "        dTreePerf = [['Depth', 'Accuracy', 'Precision', 'F1']]\n",
    "        params = dict(max_depth=TREE_DEPTH)\n",
    "        grid = GridSearchCV(DecisionTreeClassifier(), param_grid=params, cv=validation, n_jobs=-1, iid=True, scoring={'accuracy', 'precision', 'f1'}, refit='accuracy')\n",
    "\n",
    "        #Fit data to Decision Tree algo\n",
    "        grid.fit(features, answers)\n",
    "\n",
    "        #Loop through results\n",
    "        for i in range(0, 4):\n",
    "            dTreePerf.append([grid.cv_results_['params'][i]['max_depth'],\n",
    "                              \"{0:.2f}\".format(grid.cv_results_['mean_test_accuracy'][i]*100),\n",
    "                              \"{0:.2f}\".format(grid.cv_results_['mean_test_precision'][i]*100),\n",
    "                              \"{0:.2f}\".format(grid.cv_results_['mean_test_f1'][i]*100)])\n",
    "\n",
    "        print(tabulate(dTreePerf, headers=\"firstrow\"))\n",
    "        print(\"\\nThe best is depth = %s\" %(grid.best_params_['max_depth']))\n",
    "        print()\n",
    "        validationCounter += 1\n",
    "    \n",
    "    print(str(validationCounter)+\".Training best params with 5-fold cross-validation\\n\")\n",
    "    dTreePerf = [['Depth', 'Accuracy', 'Precision', 'F1']]\n",
    "    params = dict(max_depth=[grid.best_params_['max_depth']])\n",
    "    bestGrid = GridSearchCV(DecisionTreeClassifier(), param_grid=params, cv=10, n_jobs=-1, iid=True, scoring={'accuracy', 'precision', 'f1'}, refit='accuracy')\n",
    "    \n",
    "    #Fit data to Decision Tree algo\n",
    "    bestGrid.fit(features, answers)\n",
    "    \n",
    "    dTreePerf.append([bestGrid.cv_results_['params'][0]['max_depth'],\n",
    "                      \"{0:.2f}\".format(bestGrid.cv_results_['mean_test_accuracy'][0]*100),\n",
    "                      \"{0:.2f}\".format(bestGrid.cv_results_['mean_test_precision'][0]*100),\n",
    "                      \"{0:.2f}\".format(bestGrid.cv_results_['mean_test_f1'][0]*100)])\n",
    "    \n",
    "    print(tabulate(dTreePerf, headers=\"firstrow\"))\n",
    "    print(\"-> Done\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn():\n",
    "    \"\"\"\n",
    "    Algorithme 'KNN' utilisé pour classer les données qui lui sont fourni\n",
    "    Args:\n",
    "        --\n",
    "    \"\"\"\n",
    "    validationCounter = 1\n",
    "    for method in VALIDATION_METHOD:\n",
    "        validation =  None\n",
    "        if method == 'Holdout':\n",
    "            validation = ShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
    "        elif method == 'Stratified Shuffle Split':\n",
    "            validation = StratifiedShuffleSplit()\n",
    "            \n",
    "        print(str(validationCounter)+\".Training with \"+method+\"\\n\")\n",
    "        knnPerf = [['Weights', 'K', 'Accuracy', 'Precision', 'F1']]\n",
    "        params = dict(n_neighbors=N_NEIGHBORS, weights=WEIGHTS, algorithm=['auto'])\n",
    "        holdoutValidation = ShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
    "        grid = GridSearchCV(KNeighborsClassifier(), param_grid=params, cv=validation, n_jobs=-1, iid=True, scoring={'accuracy', 'precision', 'f1'}, refit='accuracy')\n",
    "\n",
    "        #Fit data to knn algo\n",
    "        grid.fit(features, answers)\n",
    "\n",
    "        #Loop through results\n",
    "        for i in range(0, 6):\n",
    "            knnPerf.append([grid.cv_results_['params'][i]['weights'],\n",
    "                            grid.cv_results_['params'][i]['n_neighbors'],\n",
    "                            \"{0:.2f}\".format(grid.cv_results_['mean_test_accuracy'][i]*100),\n",
    "                            \"{0:.2f}\".format(grid.cv_results_['mean_test_precision'][i]*100),\n",
    "                            \"{0:.2f}\".format(grid.cv_results_['mean_test_f1'][i]*100)])\n",
    "\n",
    "        print(tabulate(knnPerf, headers=\"firstrow\"))\n",
    "        print(\"\\nThe best is KNN %s With K = %s\" %(grid.best_params_['weights'], grid.best_params_['n_neighbors']))\n",
    "        print()\n",
    "        validationCounter += 1\n",
    "    \n",
    "    print(\"\\n\"+str(validationCounter)+\".Training best params with 5-fold cross-validation\\n\")\n",
    "    knnPerf = [['Weights', 'K', 'Accuracy', 'Precision', 'F1']]\n",
    "    params = dict(n_neighbors=[grid.best_params_['n_neighbors']], weights=[grid.best_params_['weights']], algorithm=['auto'])\n",
    "    bestGrid = GridSearchCV(KNeighborsClassifier(), param_grid=params, cv=10, n_jobs=-1, iid=True, scoring={'accuracy', 'precision', 'f1'}, refit='accuracy')\n",
    "    \n",
    "    #Fit data to knn algo\n",
    "    bestGrid.fit(features, answers)\n",
    "    \n",
    "    knnPerf.append([bestGrid.cv_results_['params'][0]['weights'],\n",
    "                    bestGrid.cv_results_['params'][0]['n_neighbors'],\n",
    "                    \"{0:.2f}\".format(bestGrid.cv_results_['mean_test_accuracy'][0]*100),\n",
    "                    \"{0:.2f}\".format(bestGrid.cv_results_['mean_test_precision'][0]*100),\n",
    "                    \"{0:.2f}\".format(bestGrid.cv_results_['mean_test_f1'][0]*100)])\n",
    "    \n",
    "    print(tabulate(knnPerf, headers=\"firstrow\"))\n",
    "    print(\"-> Done\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes (MultinomialMB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayes():\n",
    "    validationCounter = 1\n",
    "    for method in VALIDATION_METHOD:\n",
    "        validation =  None\n",
    "        if method == 'Holdout':\n",
    "            validation = ShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
    "        elif method == 'Stratified Shuffle Split':\n",
    "            validation = StratifiedShuffleSplit()\n",
    "            \n",
    "        print(str(validationCounter)+\".Training with \"+method+\"\\n\")\n",
    "        bayesPerf = [['Data type', 'Accuracy', 'Precision', 'F1']]\n",
    "        params = dict()\n",
    "        holdoutValidation = ShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
    "        grid = GridSearchCV(MultinomialNB(), param_grid=params, cv=validation, n_jobs=-1, iid=True, scoring={'accuracy', 'precision', 'f1'}, refit='accuracy')\n",
    "\n",
    "        #Scale the data between 0 and 1\n",
    "        scaler = preprocessing.MinMaxScaler()\n",
    "        scaler.fit(features)\n",
    "        xNormalized = scaler.transform(features)\n",
    "\n",
    "        #Fit normalized data to Bayes algo\n",
    "        grid.fit(xNormalized, answers)\n",
    "        bayesPerf.append(['Normalized',\n",
    "                          \"{0:.2f}\".format(float(grid.cv_results_['mean_test_accuracy'])*100),\n",
    "                          \"{0:.2f}\".format(float(grid.cv_results_['mean_test_precision'])*100),\n",
    "                          \"{0:.2f}\".format(float(grid.cv_results_['mean_test_f1'])*100)])\n",
    "        normalizedResult = float(grid.cv_results_['mean_test_accuracy'])\n",
    "\n",
    "        #Discretize normalized data\n",
    "        est = preprocessing.KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n",
    "        est.fit(features)\n",
    "        xKBinsDiscretizer = est.transform(features)\n",
    "\n",
    "        #Fit discretize data to bayes algorithm\n",
    "        grid.fit(xKBinsDiscretizer, answers)\n",
    "        bayesPerf.append(['Discretized',\n",
    "                            \"{0:.2f}\".format(float(grid.cv_results_['mean_test_accuracy'])*100),\n",
    "                            \"{0:.2f}\".format(float(grid.cv_results_['mean_test_precision'])*100),\n",
    "                            \"{0:.2f}\".format(float(grid.cv_results_['mean_test_f1'])*100)])\n",
    "        discretizedResult = float(grid.cv_results_['mean_test_accuracy'])\n",
    "\n",
    "        print(tabulate(bayesPerf, headers=\"firstrow\"))\n",
    "        if normalizedResult > discretizedResult:\n",
    "            print(\"\\nThe best is KNN %s With K = %s\" %(grid.best_params_['weights'], grid.best_params_['n_neighbors']))\n",
    "        print()\n",
    "        validationCounter += 1\n",
    "    \n",
    "    print(\"\\n\"+str(validationCounter)+\".Training best params with 5-fold cross-validation\\n\")\n",
    "    bayesPerf = [['Data type', 'Accuracy', 'Precision', 'F1']]\n",
    "    params = dict()\n",
    "    bestGrid = GridSearchCV(MultinomialNB(), param_grid=params, cv=10, n_jobs=-1, iid=True, scoring={'accuracy', 'precision', 'f1'}, refit='accuracy')\n",
    "    \n",
    "    #Fit data to Bayes algo\n",
    "    if normalizedResult > discretizedResult:\n",
    "        bestGrid.fit(xNormalized, answers)\n",
    "    else:\n",
    "        bestGrid.fit(xKBinsDiscretizer, answers)\n",
    "    \n",
    "    bayesPerf.append(['Discretized',\n",
    "                        \"{0:.2f}\".format(float(bestGrid.cv_results_['mean_test_accuracy'])*100),\n",
    "                        \"{0:.2f}\".format(float(bestGrid.cv_results_['mean_test_precision'])*100),\n",
    "                        \"{0:.2f}\".format(float(bestGrid.cv_results_['mean_test_f1'])*100)])\n",
    "    \n",
    "    print(tabulate(bayesPerf, headers=\"firstrow\"))\n",
    "    print(\"-> Done\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lab2_prepareDataset(datasetName, dataset):\n",
    "    print(\"PREPARING DATASETS\")\n",
    "    allData_length = len(list(csv.reader(open(dataset))))\n",
    "    progress = 0\n",
    "    datas = []\n",
    "\n",
    "    print(\"Reading \" + datasetName + \" features:\")\n",
    "    utilities.printProgressBar(0, allData_length, prefix='Progress:', suffix='Complete', length=50)\n",
    "    with open(dataset, 'r') as theFile:\n",
    "        primitives = csv.reader(theFile, delimiter=',', quotechar='|')\n",
    "\n",
    "        for row in primitives:\n",
    "            progress += 1\n",
    "            utilities.printProgressBar(progress+1, allData_length, prefix='Progress', suffix='Complete', length=50)\n",
    "\n",
    "            values = [float(i) for i in row]\n",
    "            if datasetName == \"Galaxy\":\n",
    "                datas.append(imageFeat(values))\n",
    "            elif datasetName == \"Spam\":\n",
    "                datas.append(Spam(values))\n",
    "    print(\"\\n-> Done\\n\")\n",
    "\n",
    "    #3. Split dataset using model_selection\n",
    "    for data in np.array(datas):\n",
    "        features.append(data.features)\n",
    "        answers.append(data.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREPARING DATASETS\n",
      "Reading Galaxy features:\n",
      "Progress |**************************************************| 100.0% Complete\n",
      "-> Done\n",
      "\n",
      "ALGORITHMS\n",
      "\n",
      "Decision Tree:\n",
      "1.Training with Holdout\n",
      "\n",
      "  Depth    Accuracy    Precision     F1\n",
      "-------  ----------  -----------  -----\n",
      "              90.92        91.31  91.28\n",
      "      3       86.55        86.04  87.27\n",
      "      5       89.98        91.34  90.26\n",
      "     10       91.99        92.52  92.29\n",
      "\n",
      "The best is depth = 10\n",
      "2.Training with Stratified Shuffle Split\n",
      "\n",
      "  Depth    Accuracy    Precision     F1\n",
      "-------  ----------  -----------  -----\n",
      "              90.46        91.01  90.79\n",
      "      3       85.88        85.12  86.65\n",
      "      5       89.34        91.27  89.55\n",
      "     10       91.27        91.67  91.58\n",
      "\n",
      "The best is depth = 10\n",
      "\n",
      "3.Training best params with 5-fold cross-validation\n",
      "\n",
      "  Depth    Accuracy    Precision     F1\n",
      "-------  ----------  -----------  -----\n",
      "     10       91.47        91.89  91.77\n",
      "-> Done\n",
      "\n",
      "\n",
      "KNN:\n",
      "1.Training with Holdout\n",
      "\n",
      "Weights      K    Accuracy    Precision     F1\n",
      "---------  ---  ----------  -----------  -----\n",
      "uniform      3       59.2         61.12  60.28\n",
      "distance     3       58.81        60.97  59.49\n",
      "uniform      5       60.2         61.9   61.59\n",
      "distance     5       59.67        61.63  60.67\n",
      "uniform     10       59.76        62.97  58.8\n",
      "distance    10       60.7         62.33  62.17\n",
      "\n",
      "The best is KNN distance With K = 10\n",
      "2.Training with Stratified Shuffle Split\n",
      "\n",
      "Weights      K    Accuracy    Precision     F1\n",
      "---------  ---  ----------  -----------  -----\n",
      "uniform      3       59.08        60.74  60.36\n",
      "distance     3       59.25        61.05  60.24\n",
      "uniform      5       59.95        61.27  61.72\n",
      "distance     5       59.87        61.39  61.3\n",
      "uniform     10       59.18        62.12  58.21\n",
      "distance    10       60.25        61.4   62.25\n",
      "\n",
      "The best is KNN distance With K = 10\n",
      "\n",
      "3.Training best params with 5-fold cross-validation\n",
      "\n",
      "Weights      K    Accuracy    Precision     F1\n",
      "---------  ---  ----------  -----------  -----\n",
      "distance    10       60.21        61.52  61.93\n",
      "-> Done\n",
      "\n",
      "\n",
      "\n",
      "Bayes MultinomialNB\n",
      "1.Training with Holdout\n",
      "\n",
      "Data type      Accuracy    Precision     F1\n",
      "-----------  ----------  -----------  -----\n",
      "Normalized        81.14        80.9   82.17\n",
      "Discretized       75.64        79.56  75.37\n",
      "2.Training with Stratified Shuffle Split\n",
      "\n",
      "Data type      Accuracy    Precision     F1\n",
      "-----------  ----------  -----------  -----\n",
      "Normalized        79.59        79.16  80.74\n",
      "Discretized       75.98        79.9   75.64\n",
      "\n",
      "3.Training best params with 5-fold cross-validation\n",
      "\n",
      "Data type      Accuracy    Precision     F1\n",
      "-----------  ----------  -----------  -----\n",
      "Discretized       79.93        79.34  81.09\n",
      "-> Done\n",
      "\n",
      "\n",
      "PREPARING DATASETS\n",
      "Reading Spam features:\n",
      "Progress |**************************************************| 100.0% Complete\n",
      "-> Done\n",
      "\n",
      "ALGORITHMS\n",
      "\n",
      "Decision Tree:\n",
      "1.Training with Holdout\n",
      "\n",
      "  Depth    Accuracy    Precision     F1\n",
      "-------  ----------  -----------  -----\n",
      "              88.41        86.82  85.65\n",
      "      3       86.23        83.78  83.04\n",
      "      5       88.77        90.2   85.58\n",
      "     10       87.68        85.91  84.75\n",
      "\n",
      "The best is depth = 5\n",
      "2.Training with Stratified Shuffle Split\n",
      "\n",
      "  Depth    Accuracy    Precision     F1\n",
      "-------  ----------  -----------  -----\n",
      "              89.67        87.11  87.08\n",
      "      3       88.62        86.03  85.72\n",
      "      5       90.25        91.06  87.27\n",
      "     10       90.94        90.71  88.37\n",
      "\n",
      "The best is depth = 10\n",
      "\n",
      "3.Training best params with 5-fold cross-validation\n",
      "\n",
      "  Depth    Accuracy    Precision    F1\n",
      "-------  ----------  -----------  ----\n",
      "     10       91.63        91.33  89.3\n",
      "-> Done\n",
      "\n",
      "\n",
      "KNN:\n",
      "1.Training with Holdout\n",
      "\n",
      "Weights      K    Accuracy    Precision     F1\n",
      "---------  ---  ----------  -----------  -----\n",
      "uniform      3       76.81        72.27  71.3\n",
      "distance     3       78.44        73.99  73.5\n",
      "uniform      5       78.26        74.09  73.09\n",
      "distance     5       79.89        76.02  75.17\n",
      "uniform     10       76.09        75     68.12\n",
      "distance    10       80.62        77.93  75.63\n",
      "\n",
      "The best is KNN distance With K = 10\n",
      "2.Training with Stratified Shuffle Split\n",
      "\n",
      "Weights      K    Accuracy    Precision     F1\n",
      "---------  ---  ----------  -----------  -----\n",
      "uniform      3       78.19        73.51  72.47\n",
      "distance     3       79.86        75.51  74.69\n",
      "uniform      5       78.3         73.67  72.67\n",
      "distance     5       79.93        75.63  74.78\n",
      "uniform     10       76.7         72.97  69.79\n",
      "distance    10       79.6         74.89  74.58\n",
      "\n",
      "The best is KNN distance With K = 5\n",
      "\n",
      "3.Training best params with 5-fold cross-validation\n",
      "\n",
      "Weights      K    Accuracy    Precision     F1\n",
      "---------  ---  ----------  -----------  -----\n",
      "distance     5       80.33        75.26  75.57\n",
      "-> Done\n",
      "\n",
      "\n",
      "\n",
      "Bayes MultinomialNB\n",
      "1.Training with Holdout\n",
      "\n",
      "Data type      Accuracy    Precision    F1\n",
      "-----------  ----------  -----------  ----\n",
      "Normalized        90.04        93.4   87\n",
      "Discretized       67.93        82.67  41.2\n",
      "2.Training with Stratified Shuffle Split\n",
      "\n",
      "Data type      Accuracy    Precision     F1\n",
      "-----------  ----------  -----------  -----\n",
      "Normalized        90.43        93.09  87.32\n",
      "Discretized       68.26        79.68  40.77\n",
      "\n",
      "3.Training best params with 5-fold cross-validation\n",
      "\n",
      "Data type      Accuracy    Precision     F1\n",
      "-----------  ----------  -----------  -----\n",
      "Discretized       89.82        93.39  86.23\n",
      "-> Done\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#0. Generate new feature files for Galaxy (only run once)\n",
    "#utilities.lab1_extractFeatures()\n",
    "\n",
    "#1.A Read Galaxy features (name of file, path, n_split, test size, random state)\n",
    "features = []\n",
    "answers = []\n",
    "if os.path.isfile(MERGED_GALAXY_PRIMITIVE):\n",
    "    lab2_prepareDataset(\"Galaxy\", MERGED_GALAXY_PRIMITIVE)\n",
    "else:\n",
    "    lab2_prepareDataset(\"Galaxy\", ALL_GALAXY_PRIMITIVE)\n",
    "\n",
    "#2.A Execute Algorithm\n",
    "print(\"ALGORITHMS\")\n",
    "#2.1. DECISION TREE\n",
    "print(\"\\nDecision Tree:\")\n",
    "decisionTree()\n",
    "#2.2. KNN\n",
    "print(\"KNN:\")\n",
    "knn()\n",
    "#2.3. BAYES\n",
    "print(\"\\nBayes MultinomialNB\")\n",
    "bayes()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#3.B Read Spams features (name of file, path, n_split, test size, random state)\n",
    "features = []\n",
    "answers = []\n",
    "lab2_prepareDataset(\"Spam\", ALL_SPAM_PRIMITIVE)\n",
    "\n",
    "#4.B Execute Algorithm\n",
    "print(\"ALGORITHMS\")\n",
    "#4.1. DECISION TREE\n",
    "print(\"\\nDecision Tree:\")\n",
    "decisionTree()\n",
    "#4.2. KNN\n",
    "print(\"KNN:\")\n",
    "knn()\n",
    "#4.3. BAYES\n",
    "print(\"\\nBayes MultinomialNB\")\n",
    "bayes()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*to delete\n",
    "résultat des algo sans rajouté de nos premitive\n",
    "KNN uniform has a score of 0.68 with k=3 \n",
    "KNN distance has a score of 0.69 with k=3 \n",
    "KNN uniform has a score of 0.64 with k=5 \n",
    "KNN distance has a score of 0.65 with k=5 \n",
    "KNN uniform has a score of 0.69 with k=10 \n",
    "KNN distance has a score of 0.67 with k=10 \n",
    "The best parameters are {'algorithm': 'auto', 'n_neighbors': 3, 'weights': 'distance'} with a score of 0.69\n",
    "MultinomialNB with normalized values best score is 0.89\n",
    "MultinomialNB with discretized values best score is 0.77\n",
    "Working with the images\n",
    "KNN uniform has a score of 0.61 with k=3 \n",
    "KNN distance has a score of 0.59 with k=3 \n",
    "KNN uniform has a score of 0.62 with k=5 \n",
    "KNN distance has a score of 0.60 with k=5 \n",
    "KNN uniform has a score of 0.62 with k=10 \n",
    "KNN distance has a score of 0.63 with k=10 \n",
    "The best parameters are {'algorithm': 'auto', 'n_neighbors': 10, 'weights': 'distance'} with a score of 0.63\n",
    "MultinomialNB with normalized values best score is 0.71\n",
    "MultinomialNB with discretized values best score is 0.71"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "### Méthode de création des ensembles de données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "### Détails des ensembles produits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "### Approche de validation proposée et justification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "### Matrice des expérimentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "### Étude des hyperparamètres et des modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "### Impact de la taille des ensembles de données sur la performance de classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "### Impact du bruit dans les ensembles de données sur la performance de classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "### Discussion sur la nature des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "### Recommandations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "### Améliorations possibles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliographie"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
