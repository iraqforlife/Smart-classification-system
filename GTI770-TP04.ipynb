{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratoire 4 : Développement d'un système intelligent\n",
    "#### Département du génie logiciel et des technologies de l’information\n",
    "\n",
    "| Étudiants             |                                                         |\n",
    "|-----------------------|---------------------------------------------------------|\n",
    "| Jean-Philippe Decoste |  DECJ19059105                                           |\n",
    "| Ahmad Al-Taher        |   ALTA22109307                                          |\n",
    "| Stéphanie Lacerte     |   LACS06629109                                          |\n",
    "| Cours                 | GTI770 - Systèmes intelligents et apprentissage machine |\n",
    "| Session               | Automne 2018                                            |\n",
    "| Groupe                | 2                                                       |\n",
    "| Numéro du laboratoire | 04                                                      |\n",
    "| Professeur            | Hervé Lombaert                                          |\n",
    "| Chargé de laboratoire | Pierre-Luc Delisle                                      |\n",
    "| Date                  | 13 dev 2018                                             |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from algos import DecisionTree\n",
    "from algos import Knn\n",
    "from algos import Bayes\n",
    "from algos import NeuralNetwork\n",
    "from algos import SVM\n",
    "from helpers import datasets as Data\n",
    "from helpers import utilities as Utils\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Music CSV\n",
    "JMIRMFCC_CSV = r\"music\\tagged_feature_sets\\msd-jmirmfccs_dev\\msd-jmirmfccs_dev.csv\"\n",
    "MARSYAS_CSV = r\"music\\tagged_feature_sets\\msd-marsyas_dev_new\\msd-marsyas_dev_new.csv\"\n",
    "RYTHMHISTOGRAM_CSV = r\"music\\tagged_feature_sets\\msd-rh_dev_new\\msd-rh_dev_new.csv\"\n",
    "\n",
    "UNTAGGED_PRIMITIVE1 = r\"music\\untagged_feature_sets\\msd-jmirmfccs_test_nolabels\\msd-jmirmfccs_test_nolabels.csv\"\n",
    "UNTAGGED_PRIMITIVE2 = r\"music\\untagged_feature_sets\\msd-rh_test_nolabels\\msd-rh_test_nolabels.csv\"\n",
    "UNTAGGED_PRIMITIVE3 = r\"music\\untagged_feature_sets\\msd-trh_test_nolabels\\msd-trh_test_nolabels.csv\"\n",
    "\n",
    "allFeatures = []\n",
    "allAnswers = []\n",
    "\n",
    "features_train = []\n",
    "answers_train = []\n",
    "features_test = []\n",
    "answers_test = []\n",
    "\n",
    "BASELINE_CLASSIFIER = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Préparation des fichiers de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREPARING DATASETS\n",
      "Reading Jmir MFCC features:\n",
      "Progress |**************************************************| 100.0% Complete\n",
      "Scaling data...\n",
      "Reducing dimension...\n",
      "Encoding labels...\n",
      "\n",
      "-> Done\n",
      "\n",
      "Splitting Dataset according to these params:\n",
      "Property        Value\n",
      "------------  -------\n",
      "n_splits          2\n",
      "test_size         0.8\n",
      "random_state      0\n",
      "-> Done\n",
      "\n",
      "\n",
      "Reading Marsyas features:\n",
      "Progress |**************************************************| 100.0% Complete\n",
      "Scaling data...\n",
      "Reducing dimension...\n",
      "Encoding labels...\n",
      "\n",
      "-> Done\n",
      "\n",
      "Reading Rythm Histogram features:\n",
      "Progress |**************************************************| 100.0% Complete\n",
      "Scaling data...\n",
      "Reducing dimension...\n",
      "Encoding labels...\n",
      "\n",
      "-> Done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1.A Prepare datasets\n",
    "print(\"PREPARING DATASETS\")\n",
    "features1, answers1 = Data.prepareTrainDataset(\"Jmir MFCC\", JMIRMFCC_CSV, True)\n",
    "features1_train, answers1_train, features1_test, answers1_test, train_indexes, test_indexes = Data.splitDataSet(features1, answers1, 2, 0.8, 0)\n",
    "\n",
    "features2, answers2 = Data.prepareTrainDataset(\"Marsyas\", MARSYAS_CSV, True)\n",
    "#features2_train, answers2_train, features2_test, answers2_test = Data.splitDataSet(features2, answers2, 2, 0.8, 0)\n",
    "trian_size = len(train_indexes)\n",
    "#slicing to keep only 20%\n",
    "test_indexes = test_indexes[0:trian_size]\n",
    "\n",
    "features2_train = features2[train_indexes]\n",
    "answers2_train = answers2[train_indexes]\n",
    "features2_test = features2[test_indexes]\n",
    "answers2_test = answers2[test_indexes]\n",
    "\n",
    "features3, answers3 = Data.prepareTrainDataset(\"Rythm Histogram\", RYTHMHISTOGRAM_CSV, True)\n",
    "#features3_train, answers3_train, features3_test, answers3_test = Data.splitDataSet(features3, answers3, 2, 0.8, 0)\n",
    "features3_train = features3[train_indexes]\n",
    "answers3_train = answers3[train_indexes]\n",
    "features3_test = features3[test_indexes]\n",
    "answers3_test = answers3[test_indexes]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Évaluation du classificateur de base\n",
    "\n",
    "Les cinq algorithmes vue pendant les laboratoires sont prêt à être testés. Simplement l'ajouter dans la liste et la transmettre à l'analyseur. \n",
    "- Arbre de décision : <i>decisionTree</i>\n",
    "- Plus proche voisin : <i>knn</i>\n",
    "- Bayes : <i>bayes</i>\n",
    "- Réseau de neurones : <i>neuralNetwork</i>\n",
    "- Machine à vecteur : <i>SVM</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train following algo to find baseLine: \n",
      "['decisionTree', 'knn', 'neuralNetwork']\n",
      "\n",
      "New Decision Tree Classifier\n",
      "1.Evaluation \n",
      "\n",
      "  Depth    Accuracy\n",
      "-------  ----------\n",
      "      5       14.42\n",
      "     10       15.62\n",
      "     15       13.23\n",
      "The best parameters are  {'max_depth': 10}  with a score of 15.62%\n",
      "\n",
      "-> Done\n",
      "\n",
      "\n",
      "\n",
      "New Decision Tree Classifier\n",
      "1.Training \n",
      "\n",
      "  Depth    Accuracy\n",
      "-------  ----------\n",
      "     10       26.06\n",
      "\n",
      "-> Done\n",
      "\n",
      "\n",
      "\n",
      "New Knn Classifier\n",
      "1.Evaluation \n",
      "\n",
      "Weights      K    Accuracy\n",
      "---------  ---  ----------\n",
      "distance     5       16.95\n",
      "distance    10       18.31\n",
      "distance    20       19.34\n",
      "The best parameters are  {'n_neighbors': 20, 'algorithm': 'auto', 'weights': 'distance'}  with a score of 19.34%\n",
      "\n",
      "-> Done\n",
      "\n",
      "\n",
      "\n",
      "New Knn Classifier\n",
      "1.Training \n",
      "\n",
      "Weights      K    Accuracy\n",
      "---------  ---  ----------\n",
      "distance    20       99.94\n",
      "\n",
      "-> Done\n",
      "\n",
      "\n",
      "\n",
      "New Neural Network Classifier\n",
      "1.Evaluation\n",
      "\n",
      "  Epoch    Batch Size    Accuracy\n",
      "-------  ------------  ----------\n",
      "    250          1000       96.01\n",
      "    800          1000       96.01\n",
      "    250          5000       96.02\n",
      "    800          5000       96.02\n",
      "The best parameters are  {'activation': ['relu', 'softmax'], 'learn_rate': 0.01, 'layers': [25, 30, 15], 'batch_size': 5000, 'epochs': 800}  with a score of 96.02%\n",
      "\n",
      "-> Done\n",
      "\n",
      "\n",
      "\n",
      "New Neural Network Classifier\n",
      "1.Training\n",
      "  Epoch    Batch Size    Accuracy\n",
      "-------  ------------  ----------\n",
      "    800          5000          96\n",
      "\n",
      "Train following algo to find baseLine: \n",
      "['decisionTree', 'knn', 'neuralNetwork']\n",
      "\n",
      "New Decision Tree Classifier\n",
      "1.Evaluation \n",
      "\n",
      "  Depth    Accuracy\n",
      "-------  ----------\n",
      "      5       16.2\n",
      "     10       17.46\n",
      "     15       15.06\n",
      "The best parameters are  {'max_depth': 10}  with a score of 17.46%\n",
      "\n",
      "-> Done\n",
      "\n",
      "\n",
      "\n",
      "New Decision Tree Classifier\n",
      "1.Training \n",
      "\n",
      "  Depth    Accuracy\n",
      "-------  ----------\n",
      "     10       27.61\n",
      "\n",
      "-> Done\n",
      "\n",
      "\n",
      "\n",
      "New Knn Classifier\n",
      "1.Evaluation \n",
      "\n",
      "Weights      K    Accuracy\n",
      "---------  ---  ----------\n",
      "distance     5       20.72\n",
      "distance    10       22.17\n",
      "distance    20       23.09\n",
      "The best parameters are  {'n_neighbors': 20, 'algorithm': 'auto', 'weights': 'distance'}  with a score of 23.09%\n",
      "\n",
      "-> Done\n",
      "\n",
      "\n",
      "\n",
      "New Knn Classifier\n",
      "1.Training \n",
      "\n",
      "Weights      K    Accuracy\n",
      "---------  ---  ----------\n",
      "distance    20       99.94\n",
      "\n",
      "-> Done\n",
      "\n",
      "\n",
      "\n",
      "New Neural Network Classifier\n",
      "1.Evaluation\n",
      "\n",
      "  Epoch    Batch Size    Accuracy\n",
      "-------  ------------  ----------\n",
      "    250          1000       96.11\n",
      "    800          1000       96.1\n",
      "    250          5000       96.11\n",
      "    800          5000       96.11\n",
      "The best parameters are  {'activation': ['relu', 'softmax'], 'learn_rate': 0.01, 'layers': [25, 30, 15], 'batch_size': 5000, 'epochs': 250}  with a score of 96.11%\n",
      "\n",
      "-> Done\n",
      "\n",
      "\n",
      "\n",
      "New Neural Network Classifier\n",
      "1.Training\n",
      "  Epoch    Batch Size    Accuracy\n",
      "-------  ------------  ----------\n",
      "    250          5000       96.07\n",
      "\n",
      "Train following algo to find baseLine: \n",
      "['decisionTree', 'knn', 'neuralNetwork']\n",
      "\n",
      "New Decision Tree Classifier\n",
      "1.Evaluation \n",
      "\n",
      "  Depth    Accuracy\n",
      "-------  ----------\n",
      "      5       13.11\n",
      "     10       13.45\n",
      "     15       11.49\n",
      "The best parameters are  {'max_depth': 10}  with a score of 13.45%\n",
      "\n",
      "-> Done\n",
      "\n",
      "\n",
      "\n",
      "New Decision Tree Classifier\n",
      "1.Training \n",
      "\n",
      "  Depth    Accuracy\n",
      "-------  ----------\n",
      "     10       20.82\n",
      "\n",
      "-> Done\n",
      "\n",
      "\n",
      "\n",
      "New Knn Classifier\n",
      "1.Evaluation \n",
      "\n",
      "Weights      K    Accuracy\n",
      "---------  ---  ----------\n",
      "distance     5       12.53\n",
      "distance    10       13.98\n",
      "distance    20       14.73\n",
      "The best parameters are  {'n_neighbors': 20, 'algorithm': 'auto', 'weights': 'distance'}  with a score of 14.73%\n",
      "\n",
      "-> Done\n",
      "\n",
      "\n",
      "\n",
      "New Knn Classifier\n",
      "1.Training \n",
      "\n",
      "Weights      K    Accuracy\n",
      "---------  ---  ----------\n",
      "distance    20        99.9\n",
      "\n",
      "-> Done\n",
      "\n",
      "\n",
      "\n",
      "New Neural Network Classifier\n",
      "1.Evaluation\n",
      "\n",
      "  Epoch    Batch Size    Accuracy\n",
      "-------  ------------  ----------\n",
      "    250          1000       96.05\n",
      "    800          1000       96.05\n",
      "    250          5000       96.06\n",
      "    800          5000       96.05\n",
      "The best parameters are  {'activation': ['relu', 'softmax'], 'learn_rate': 0.01, 'layers': [25, 30, 15], 'batch_size': 5000, 'epochs': 250}  with a score of 96.06%\n",
      "\n",
      "-> Done\n",
      "\n",
      "\n",
      "\n",
      "New Neural Network Classifier\n",
      "1.Training\n",
      "  Epoch    Batch Size    Accuracy\n",
      "-------  ------------  ----------\n",
      "    250          5000       96.03\n",
      "\n",
      "The baseline Classifier is:\n",
      "['Plus proche voisin (KNN):', []]\n"
     ]
    }
   ],
   "source": [
    "#2.A Find best algo for specified dataset\n",
    "algo_to_test = ['decisionTree', 'knn', 'neuralNetwork']\n",
    "best_data1 = Utils.findBaselineClassifier(algo_to_test, features1_train, answers1_train)\n",
    "best_data2 = Utils.findBaselineClassifier(algo_to_test, features2_train, answers2_train)\n",
    "best_data3 = Utils.findBaselineClassifier(algo_to_test, features3_train, answers3_train)\n",
    "#2.B Find best algo as Baseline Classifier\n",
    "BASELINE_CLASSIFIER = Utils.getBestModel([best_data1, best_data2, best_data3])\n",
    "print(\"The baseline Classifier is:\")\n",
    "print(BASELINE_CLASSIFIER.getDefinition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting Dataset according to these params:\n",
      "Property        Value\n",
      "------------  -------\n",
      "n_splits          2\n",
      "test_size         0.6\n",
      "random_state      0\n",
      "-> Done\n",
      "\n",
      "\n",
      "\n",
      "New Decision Tree Classifier\n",
      "\n",
      "New Knn Classifier\n",
      "\n",
      "New Neural Network Classifier\n",
      "1.Training \n",
      "\n",
      "  Depth    Accuracy\n",
      "-------  ----------\n",
      "     10        23.1\n",
      "\n",
      "-> Done\n",
      "\n",
      "\n",
      "1.Training \n",
      "\n",
      "Weights      K    Accuracy\n",
      "---------  ---  ----------\n",
      "distance    20       99.94\n",
      "\n",
      "-> Done\n",
      "\n",
      "\n",
      "1.Training\n",
      "Epoch       Batch Size      Accuracy\n",
      "----------  ------------  ----------\n",
      "[250, 800]  [1000, 5000]          96\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'features_train,answers_train,features_test,answers_test = Data.splitDataSet(allFeatures,allAnswers, 10, 0.99, 0)\\ntestF,testA,dummy2,dummy1 = Data.splitDataSet(features_test,answers_test, 1, 0.99, 0)\\nprint(\"features_train %d; answers_train %d; features_test %d; answers_test %d\"%(len(features_train),len(answers_train),len(testF),len(testA)))\\n\\nmodel1 = KNeighborsClassifier(n_neighbors=3)\\nmodel2 = KNeighborsClassifier(n_neighbors=5)\\nmodel3 = KNeighborsClassifier(n_neighbors=7)\\n#we train here\\nmodel1.fit(features_train,answers_train)\\nmodel2.fit(features_train,answers_train)\\nmodel3.fit(features_train,answers_train)\\n\\n#combine\\npredit1 = model1.predict(testF)\\npredit2 = model2.predict(testF)\\npredit3 = model3.predict(testF)\\n\\n#print(\"P1 %d; P2 %d; P3 %d\" %(predit1,predit2,predit3))\\npredictedAnswers = []\\n#\\nfor index in range(0,len(predit1)):\\n    if predit1[index] == predit2[index] or predit1 [index]== predit3[index]:\\n        predictedAnswers.append(predit1[index])\\n    elif predit2[index] == predit3[index]:\\n        predictedAnswers.append(predit2[index])\\n    else:\\n        #base on best score\\n        predictedAnswers.append(predit3[index])\\n    '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3.A Resplit dataset with a little more data\n",
    "features1_train, answers1_train, features1_test, answers1_test = Data.splitDataSet(features1, answers1, 2, 0.6, 0)\n",
    "#3.B Prepare algo combination\n",
    "dTree = DecisionTree.decisionTree(5, 10)\n",
    "knn = Knn.knn(5, 20, 'distance')\n",
    "neuralNetwork = NeuralNetwork.neuralNetwork(2, 2, [25, 30, 15], 0.01, len(features1[0]), ['relu', 'softmax'], 800, 1000)\n",
    "\n",
    "dTree.train(features1_train, answers1_train)\n",
    "knn.train(features1_train, answers1_train)\n",
    "neuralNetwork.train(features1_train, answers1_train)\n",
    "\n",
    "#allFeatures,allAnswers, labelEncoder = Data.prepareTrainDataset(\"Music\", TAGGED_PRIMITIVE)\n",
    "#testF1, ids = Data.prepareTestDataset(\"Music\", UNTAGGED_PRIMITIVE1)\n",
    "#testF2, ids = Data.prepareTrainDataset(\"Music\", UNTAGGED_PRIMITIVE2)\n",
    "#testF3, ids = Data.prepareTrainDataset(\"Music\", UNTAGGED_PRIMITIVE3)\n",
    "#transform back the answers from numeric to label\n",
    "\"\"\"\"answers = []\n",
    "for index in range(0,len(ids)):\n",
    "    answers.append(random.randint(1,14))\n",
    "labels = labelEncoder.inverse_transform(answers)\n",
    "Data.outPut(ids,labels)\"\"\"\n",
    "#determine best dementions to use\n",
    "#faster tests\n",
    "#transformed =  decomposition.PCA(n_components=5).fit_transform(allFeatures)\n",
    "#determine best classifier and best model\n",
    "'''features_train,answers_train,features_test,answers_test = Data.splitDataSet(allFeatures,allAnswers, 10, 0.99, 0)\n",
    "testF,testA,dummy2,dummy1 = Data.splitDataSet(features_test,answers_test, 1, 0.99, 0)\n",
    "print(\"features_train %d; answers_train %d; features_test %d; answers_test %d\"%(len(features_train),len(answers_train),len(testF),len(testA)))\n",
    "\n",
    "model1 = KNeighborsClassifier(n_neighbors=3)\n",
    "model2 = KNeighborsClassifier(n_neighbors=5)\n",
    "model3 = KNeighborsClassifier(n_neighbors=7)\n",
    "#we train here\n",
    "model1.fit(features_train,answers_train)\n",
    "model2.fit(features_train,answers_train)\n",
    "model3.fit(features_train,answers_train)\n",
    "\n",
    "#combine\n",
    "predit1 = model1.predict(testF)\n",
    "predit2 = model2.predict(testF)\n",
    "predit3 = model3.predict(testF)\n",
    "\n",
    "#print(\"P1 %d; P2 %d; P3 %d\" %(predit1,predit2,predit3))\n",
    "predictedAnswers = []\n",
    "#\n",
    "for index in range(0,len(predit1)):\n",
    "    if predit1[index] == predit2[index] or predit1 [index]== predit3[index]:\n",
    "        predictedAnswers.append(predit1[index])\n",
    "    elif predit2[index] == predit3[index]:\n",
    "        predictedAnswers.append(predit2[index])\n",
    "    else:\n",
    "        #base on best score\n",
    "        predictedAnswers.append(predit3[index])\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction et revue de la littérature (Question 1)\n",
    "\n",
    "Plusieurs techniques sont en places depuis quelques années déjà afin de classifier les différents sons et musique. Des descripteurs cepstraux sont souvent utilisés pour le traitement de la parole par exemple. Selon la thèse produite par Slim Essid portant sur la classification automatique des signaux audiofréquences : reconnaissance des instruments de musique, la représentation cepstrale s’avère efficace pour de nombreuses tâches de classification audio telles que la discrimination parole/musique, la reconnaissance du genre ou encore la reconnaissance des instruments.\n",
    "\n",
    "Dans le cadre du laboratoire, les Mel-Frequency Cepstral Coefficients (MFCC) sont justement sujets à classification. Mentionné dans plusieurs tel que Pachet - Cazaly en 2000, McKay et Fujinaga en 2004 ainsi que Li et Ogihara en 2005, la classification, spécifiquement hiérarchique a eu un grand succès pour les tâches de classification audio et du genre musical. La classification hiérarchique a pour but d'améliorer les taux de reconnaissance par rapport à ceux obtenus avec les systèmes dits “plats”, dans lesquels toutes les classes sont considérées sur un seul niveau, sans organisation particulière. De plus, l’application de regroupement (clustering) hiérarchique a également fait ses preuves pour la classification d’instruments non connus à l'étape d’apprentissage. La distance, tel que vu avec le k-mean dans le cours sur le regroupement, est utilisée comme critère de proximité des classes dans le processus de clustering, en faisant l'hypothèse de gaussianité des données.\n",
    "\n",
    "En terme de statistique, il est possible d'observer différents résultats selon les articles lus. En effet, le taux de classification dépend des modèles utilisés et de leur primitive. Par exemple, obtenant un taux de précision d’environ 43% avec un réseau de neurones, l’article de Hu et Ogihara, en 2012 fait ces preuves face aux résultats de précision avec des classifications aléatoires. Hu et Ogihara obtiennent également un taux de classification en utilisant la combinaison de modèle à environ 84% ce qui est très acceptable.\n",
    "\n",
    "Dans ce présent laboratoire, l’objectif premier est de construire un système intelligent permettant de classer automatiquement des échantillons de données en utilisant la combinaison de modèles et des algorithmes d’apprentissage. Le tout se fait avec les technologies Python. Dans les sections abordées ci-dessous, les sujets de configuration de la machine, l’approche de partitionnement, le choix des primitives et les modèles d’apprentissage choisi seront expliqués. De plus, les hyperparamètres seront présentés en plus de la solution finale et les améliorations possibles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "### Configuration de l'environnement\n",
    "\n",
    "Dans le cadre de ce laboratoire, le projet final a été réalisé à l’aide du langage de programmation Python et Jupiter Notebook sous le système d’exploitation Windows. Les machines utilisées pour l'exécution se résument à un processeur Intel(R) Core(TM) i7-4770k CPU 3.40GHz ainsi que 16Go de mémoire vive.\n",
    "\n",
    "### Partitionnement des données\n",
    "\n",
    "Pour décider du classificateur de base, les algorithmes n'ont reçu que 20% des données afin d'augmenter leur exécution. Lors des tests préliminaires, ce chiffre était d'environ 50%, mais la variation du résultat d'entrainement était moindre et n'expliquait pas l'heure supplémentaire de temps de calcul.\n",
    "\n",
    "### Fichier de primitives choisie\n",
    "\n",
    "À ce qui a trait au choix de primitives (CSV), l'analyse des données, la lecture de la documentation fournit ainsi que plusieurs tests ont permi de déterminer que les fichiers qui permettront aux algorithmes de performer le mieux sont les suivants: JMIR MFCCs, MARSYAS ainsi que Rythm Histogram\n",
    "\n",
    "Bien sûr, la dimension ainsi que la taille des données ont influencer cetter décision puisque le temps d'exécution des algorithmes n'est pas à négliger.\n",
    "\n",
    "### Validation\n",
    "\n",
    "La méthode de validation utilisée est StratifiedShuffleSplit. La performance des algorithmes avec cette méthode sur les ensembles de données est adéquate et évite des temps d’exécution trop long. Elle est, entre autre,  utilisé pour l'évaluation et l'entraînement du Baseline Classifier. La combinaison d’algorithme, quant à elle, est effectuée à l’aide de K-Fold cross-validation (5).\n",
    "\n",
    "### Prétraitement\n",
    "\n",
    "Afin de s'assurer d'une performance optimal, il a fallu normaliser les données en utilisant la méthode <i>MinMaxScaler</i>. Puisque le temps d'exécution était un facteur important pour l'équipe, il a également fallu réduire la dimension des données à la de PCA. De plus, KNN n'est efficace que si on lui envoi un nombre limité de dimension, cette approche offrait donc une meilleur compatibilité entre les algorithmes. Il y eu une modification supplémentaire pour optimiser le réseau neuronaux, transformer le tableau contenant les réponses en tableau binaire pour représenter chacune des classes. (EX:. avant: [4], apres: [0, 0, 0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "### Description des modèles et justifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "### Matrice des expérimentations\n",
    "\n",
    "### Matrice des résultats de l'étude des hyperparamètres\n",
    "\n",
    "### Graphiques\n",
    "\n",
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "### Présentation de la conception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "### Hyperparamètres des modèles choisis dans la conception\n",
    "\n",
    "### Matrice des résultats\n",
    "\n",
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7 \n",
    "### Formulation des recommandations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Références\n",
    "[Kitahara et al., 2004] \n",
    "T. Kitahara, M. Goto, et H.G. Okuno. Category-level identification of non-registered musical instrument sounds. Dans International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Montreal, Canada., mai 2004.\n",
    "\n",
    "[Pachet et Cazaly, 2000] \n",
    "F. Pachet et D. Cazaly. A taxonomy of musical genres. Dans Content-Based Multimedia Information Access Conference (RIAO), Paris, France, avril 2000.\n",
    "\n",
    "[McKay et Fujinaga, 2004] \n",
    "C. McKay et I. Fujinaga. Automatic genre classification using large high-level musical feature sets. Dans 5th International Conference on Music Information Retrieval (ISMIR), Barcelona, Spain, octobre 2004.\n",
    "\n",
    "[Li et Ogihara, 2005] \n",
    "Tao Li et Mitsunori Ogihara. Music genre classification with taxonomy. Dans International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Philadelphia, USA, mars 2005.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
